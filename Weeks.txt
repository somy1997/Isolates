Week 0 (Winter Vacations) :

Literature Survey - find what different commercial and open source platforms use to provide application level sandboxing

https://medium.com/@zackbloom/isolates-are-the-future-of-cloud-computing-cf7ab91c6142 = blog on isolates
https://www.cloudflare.com/products/cloudflare-workers/ = provides serverless computing architecture without using vm or containers. cloudflare workers support js only. 194 data centres. We are trying to develop something that is language agnostic.

Read the above blog on isolates.

Week 1 :

COMSNETS 2020 : No one's available.
Reading Paper SAND. Read till page 6.
Created the literature survey table on notebook, everything present in Sand.

Week 2 :

// Study the repo https://github.com/hobochild/sandy used to place custom blocks on read calls. Try to block read calls in a similar way. Understand how they are doing.
Completed reading the paper SAND. SAND mentions in chapter 9 at the end of using LWC (light weight contexts).
See if the methods mentioned in http://books.gigatux.nl/mirror/networksecuritytools/0596007949/networkst-CHP-7-SECT-2.html works? Some nuances :
1. The website asks to look at /usr/include/bits/syscall.h but it is actually present at several locations like :
    /usr/include/x86_64-linux-gnu/asm/vsyscall.h
    /usr/include/x86_64-linux-gnu/sys/syscall.h
    /usr/include/x86_64-linux-gnu/bits/syscall.h
    /usr/include/syscall.h
2. For intercept_open method, had to change the Makefile to the one in Abhijit sir's tutorial on LKM. Didn't work anyways. Showed a lot of errors. Tried with both /usr/src/linux-headers-'uname -r' and /lib/modules/'uname -r'
3. For intercept_unlink method, had to use 'sudo grep sys_call_table /boot/System.map-4.15.0-45-generic' which resulted in
    ffffffff81e001e0 R sys_call_table
    ffffffff81e015a0 R ia32_sys_call_table
After correcting some syntax errors the module was create with some warnings but while inserting it was not returning. Also, it was displaying the message 'Killed', may be because it wasn't allowed.

Week 3 : 

Completed watching Jerome's DockerCon video explaing cgroups, ... : https://youtu.be/sK5i-N34im8
Reattempting LKM tutorial
Testing method to work around 'Killed' message, following : https://stackoverflow.com/questions/59812156/how-can-i-override-a-system-call-table-entry-with-my-own-function 
Install vim and cscope : sudo apt-get update && sudo apt-get install vim cscope. Refer http://cscope.sourceforge.net/large_projects.html
The stackoverflow method checks out.
Observation : On grep, the strace method shows only a few system calls but the dmesg command shows a lot of open calls probably because the dmesg command is showing all the open calls including those called by other processes in the system.

Week 4 :

Out of station from Jan 28 to Feb 5

Week 5 :

Refer https://stackoverflow.com/questions/26451729/how-to-get-process-id-name-and-status-using-module . It has mentions of good references for newbies in kernel programming.
asmlinkage = Refer https://stackoverflow.com/questions/10459688/what-is-the-asmlinkage-modifier-meant-for , https://www.quora.com/Linux-Kernel/Linux-Kernel-What-does-asmlinkage-mean-in-the-definition-of-system-calls#
Process ID, name = https://stackoverflow.com/questions/26451729/how-to-get-process-id-name-and-status-using-module
User ID = https://stackoverflow.com/questions/14097389/how-to-get-userid-when-writing-linux-kernel-module
current in kernel = https://stackoverflow.com/questions/12434651/what-is-the-current-in-linux-kernel-source?noredirect=1&lq=1
obj-m = https://stackoverflow.com/questions/57839941/what-is-the-meaning-of-obj-m-in-linux-device-driver-makefile
make variables = https://ftp.gnu.org/old-gnu/Manuals/make-3.79.1/html_chapter/make_6.html

Used the stackoverflow method to log the calls made to open system call by defining the custom open function.
Printing the process name, process ID and user ID for each call.
Next plan to do the same thing with close, read, write syscalls as well using the same LKM.
Created a generic Makefile to take care of creating and deleting the module and insmodding and rmmodding the module. Created it with an assumption that we'll be creating separate LKMs for each syscall. So, the makefile works such as for example, if open syscall then the folder name should be open and the code should be written in a file inside the same folder named intercept_open.c .
Logs stored in format : 'ISOLATES:<syscall intercepted>,<process name>,<process id>,<user id>c
Logs stored in System Call Logging/dmesglogs.txt
Observation : In original logs logs, saw some user ids other than root(0), nbs(1000) which had id : 104
> getent passwd "104" -- syslog:x:104:108::/home/syslog:/bin/false
Probably for writing kernel logs
I think that's why the logs were filling so fast because the act of writing to logs was also getting captured in the logs.
After selectively logging the system calls, the logs were of decent size.

Week 6 :

Isolation

Week 7 :

MidSems

Logged file operations system calls like open, close, read, write.

Created new user : name : isol, pass : 1
Refer https://www.digitalocean.com/community/tutorials/how-to-add-and-delete-users-on-ubuntu-16-04 for adduser command.
Couldn't use this as it doesn't have permissions to create files in Desktop of user nbs
Working with user nbs for now.
Refer https://askubuntu.com/questions/468236/how-can-i-find-my-user-id-uid-from-terminal to find user id.

Trying to print file accessed by simple http server written in go (language chosen because it gives statically compiled binary with everything included in the binary itself).
Printing specific logs based on $UID = 1000.
While trying to print filename passed to open function, the system hangs because of __user macro in the parameter declaration which means that the parameter is in user space.
Refer https://stackoverflow.com/questions/45405442/what-is-the-meaning-of-argument-type-const-char-user-const-user-argv
Refer https://stackoverflow.com/questions/4521551/what-are-the-implications-of-the-linux-user-macro
Using copy_from_user function to copy from user space to kernel space. It worked.
Refer https://www.fsl.cs.sunysb.edu/kernel-api/re257.html
cscope -Rd to start cscope without updating the database.
Modes definition found by searching O_RDONLY using cscope. Definitions of all modes given in fcntl.h. Those accessible in kernel mentioned in linux-4.19.102/include/linux/fcntl.h
Currently watching only O_RDONLY | O_WRONLY | O_RDWR | O_CREAT | O_TRUNC | O_APPEND, NDELAY and NONBLOCK has same value as defined in fcntl.h (cscope).
Ran go executable, observed that it accessed files from /lib/...
Then statically compiled and again ran the executable, this time didn't see any mention of the server process in dmesg. May be because it is using execve, to directly execute some other process.
Digression : after compiling used strip command on the binary simplestHTTPServer to remove the symbols and reduce the file size from 5.6 MB to 5.2 MB.
Digression : Command 'sudo lsof | grep simple' used to see open files.

Next, we try to selectively block the syscalls of a process with a particular parent pid by hardcoding this pid.
The definition of task_struct can be found in /usr/src/linux-headers-4.15.0-45/include/linux/sched.h
In the definition, this was mentioned :
/*
         * Pointers to the (original) parent process, youngest child, younger sibling,
         * older sibling, respectively.  (p->father can be replaced with
         * p->real_parent->pid)
         */

        /* Real parent process: */
        struct task_struct __rcu        *real_parent;

        /* Recipient of SIGCHLD, wait4() reports: */
        struct task_struct __rcu        *parent;

Refer https://medium.com/hungys-blog/linux-kernel-process-99629d91423c , it says
Real parent vs. parent
The field parent in task_struct usually matches the process descriptor pointed by real_parent.
real_parent: Points to the process descriptor of the process that created P or to the descriptor of process 1 (init) if the parent process no longer exists.
parent: Points to the current parent of P (this is the process that must be signaled when the child process terminates, i.e. SIGCHLD). It may occasionally differ from real_parent in some cases, such as when another process issues a ptrace() system call requesting that it be allowed to monitor P.

Tested by blocking open system for all processes that are children of bash by checking parent's (real_parent) pid.
Commands like ls, dmesg were not running. The go server when built directly was not running and we saw that it was opening some library files .so outside its own folder. But, the go server (Isolates/Testing/gostaticserver/simplestHTTPServer) was running when built statically probably because it's not opening anything, just uses execve call.

Week 8 :

Next, we want to selectively log following network calls made by the static go server and python server when build from source at a location.
read, write, socket, signal, accept, connect, exit, fork, bind, : socketcall and list : http://man7.org/linux/man-pages/man2/socketcall.2.html
build python from source and in that location run : ./python -m http.server (not done)
The parameters for some of the calls weren't available in syscalls.h so had to look it up, found the declarations in socket.h file, can be searched using __sys_socket, found in linux-4.19.102/include/linux/socket.h
Digression : on cscoping __sys_accept4, found its definition in linux-.../net/socket.c and found that __sys_accept actually makes a call to __sys_accept4 itself by putting parameter for flags as 0.
Digression : Note that declaration of __sys_sendmsg, __sys_sendmmsg and __sys_recvmsg in linux-.../include/linux/socket.h was different from sys_sendmsg, sys_sendmmsg and sys_recvmsg in /usr/src/linux-headers-4.15.0-45/include/linux/syscalls.h where all syscalls are given, they had an extra bool parameter and following comment :

/* The __sys_...msg variants allow MSG_CMSG_COMPAT iff
 * forbid_cmsg_compat==false
 */
extern long __sys_sendmsg(int fd, struct user_msghdr __user *msg, unsigned int flags, bool forbid_cmsg_compat);
asmlinkage long sys_sendmsg(int fd, struct user_msghdr __user *msg, unsigned flags);

While writing the custom calls I have followed the signatures of the ones mentioned in /usr/src/.../syscalls.h
All the custom calls are defined in the order given in the list.
Do the #ifdef check when doing for every system call
Syscalls list for x86 is given here : http://asm.sourceforge.net/syscall.html

Week 9 :

Blocking all system calls available in file /usr/src/linux-headers-4.15.0-45/include/linux/syscalls.h

Observation :
There are a few syscalls based on the number of bits in the system. 
There are many syscalls whose availability depends on if certain constants are defined which i think is for backward compatibility. 
Many of the commands that we use have direct syscalls associated with them like rename, stat, etc.
Don't know the use of sysquota32ctl, couldn't find it anywhere else using cscope.
Max 6 params are allowed as visible from the #define MAP6 in the beginning of syscalls file.

Template :

asmlinkage long sys_close(unsigned int fd);

#ifdef __NR_close
static asmlinkage long custom_close(unsigned int fd)
{
    asmlinkage long (*org_close)(unsigned int fd);
    if(current->real_parent->pid == 12970)
    {
    	printk(KERN_WARNING "ISOLATES:close,%s,%d,%d\n", current->comm, current->pid, current->cred->uid.val);
    }
    org_close = (asmlinkage long (*)(unsigned int fd)) org_sys_table[__NR_close];
    return org_close(fd);
}
#endif

Regex used in gedit :

Matching syscalls : ^asmlinkage long sys_(\w+)\((.*)\);$
Replacement       : #ifdef __NR_\1\nstatic asmlinkage long custom_\1(\2)\n{\n    asmlinkage long (*org_\1)(\2);\n    if(current->real_parent->pid == PARENTPID)\n    {\n    	printk(KERN_WARNING "ISOLATES:\1,%s,%d,%d\\\\n", current->comm, current->pid, current->cred->uid.val);\n    }\n    org_\1 = (asmlinkage long (*)(\2)) org_sys_table[__NR_\1];\n}\n#endif\n
Matching the entire line for replication and repetition : (org_\w+ = .*)
Matching 0 param  : ^\s+org_(\w+) = \(asmlinkage long \(\*\)\(void\).*;$
Replacement       : \0\n    return org_\1();
Matching 1 param  : ^\s+org_(\w+) = \(asmlinkage long \(\*\)\([^,\n]* \*?(\w+)\).*;$
Replacement       : \0\n    return org_\1(\2);
The above trick might fail for next replacements when the parameter was already a single fd. Hence, for 2 params the following trick. Solved by removing return line altogether.
Matching 2 params : ^\s+org_(\w+) = \(asmlinkage long \(\*\)\([^,\n]* \*?(\w+),[^,\n]* \*?(\w+)\).*;$
Replacement       : \0\n    return org_\1(\2, \3);
Matching 3 params : ^\s+org_(\w+) = \(asmlinkage long \(\*\)\([^,\n]* \*?(\w+),[^,\n]* \*?(\w+),[^,\n]* \*?(\w+)\).*;$
Replacement       : \0\n    return org_\1(\2, \3, \4);
Matching 4 params : ^\s+org_(\w+) = \(asmlinkage long \(\*\)\([^,\n]* \*?(\w+),[^,\n]* \*?(\w+),[^,\n]* \*?(\w+),[^,\n]* \*?(\w+)\).*;$
Replacement       : \0\n    return org_\1(\2, \3, \4, \5);
Matching 5 params : ^\s+org_(\w+) = \(asmlinkage long \(\*\)\([^,\n]* \*?(\w+),[^,\n]* \*?(\w+),[^,\n]* \*?(\w+),[^,\n]* \*?(\w+),[^,\n]* \*?(\w+)\).*;$
Replacement       : \0\n    return org_\1(\2, \3, \4, \5, \6);
Matching 6 params : ^\s+org_(\w+) = \(asmlinkage long \(\*\)\([^,\n]* \*?(\w+),[^,\n]* \*?(\w+),[^,\n]* \*?(\w+),[^,\n]* \*?(\w+),[^,\n]* \*?(\w+),[^,\n]* \*?(\w+)\).*;$
Replacement       : \0\n    return org_\1(\2, \3, \4, \5, \6, \7);
Just for checking, we will try with 7 parameters
Matching 7 params : ^\s+org_(\w+) = \(asmlinkage long \(\*\)\([^,\n]* \*?(\w+),[^,\n]* \*?(\w+),[^,\n]* \*?(\w+),[^,\n]* \*?(\w+),[^,\n]* \*?(\w+),[^,\n]* \*?(\w+),[^,\n]* \*?(\w+)\).*;$

Added following to Makefile to remove unused functions warnings :
EXTRA_CFLAGS := -Wno-unused-function
For some functions couldn't find it in kernel code : got sigaction and rt_sigaction from https://linux.die.net/man/2/rt_sigaction, got uname from http://man7.org/linux/man-pages/man2/uname.2.html.
Many such syscalls appearing in error were also present in file linux-.../include/linux/compat.h with prefix compat_, found using text string search like pselect6
Functions like custom_sigaction, custom_send didn't show up in the errors even though the function wasn't written correctly still I wrote them correctly. Still many functions were left like custom_org_readdir. To not confuse org_old, need to change our prefix old_ to org_
Found sys_clone in linux-.../arch/openrisc/include/asm/syscalls.h

Regex used in gedit for init and exit modules:

Matching syscalls : ^asmlinkage long sys_(\w+)\((.*)\);$
Storing original  : #ifdef __NR_\1\n    org_sys_table[__NR_\1] = sys_call_table[__NR_\1];\n#endif
Assigning custom  : #ifdef __NR_\1\n    sys_call_table[__NR_\1] = (sys_call_ptr_t)custom_\1;\n#endif
Assigning org     : #ifdef __NR_\1\n    sys_call_table[__NR_\1] = org_sys_table[__NR_\1];\n#endif

I was thinking that since we already had all the system calls available in linux/syscalls.h, why not directly call it but then we are not sure if they are the ones actually pointed in the system call table but with our method, we are guaranteed.

Its not working : starts behaving mysteriously and hangs if doing something on terminal, and closes abruptly on rmmodding instantly

Binary searching to find the errant syscalls :
Commenting printk first to check if it works
Matching  : printk.*
Replacing : //\0\n\t\t;

Week 10 :

Rewriting the code by removing the predefined ifdefs from syscalls.h and using macros in reallops and using binary search to determine which syscalls are creating the problems.
Macros : argument x :
#x results in "x", stringifies x
##x results in x , means the argument supplied to x, helps in reading (x = open) _##x as _open instead of _x as _x, useful for concatenating tokens, if not concatenating tokens then we can directly use x
Multiple words separated by space can be given as arguments example : #define a(x) x - 5 : a(5 + 5) resolves to 5 + 5 - 5
Variable arguments can be given to macros, refer https://www.geeksforgeeks.org/variable-length-arguments-for-macros/
gcc -E shows the expanded macros, remove the include files to avoid fatal error when library not found
Regex for using macros :
#define STOREORIG(x) old_sys_table[__NR_##x] = sys_call_table[__NR_##x]
Match   : old_sys_table\[__NR_(\w+)\] = sys_call_table.*;
Replace : STOREORIG(\1);
#define APPLYCUST(x) sys_call_table[__NR_##x] = (sys_call_ptr_t)custom_##x
Match   : sys_call_table\[__NR_(\w+)\] = \(sys_call_ptr_t\).*;
Replace : APPLYCUST(\1);
#define APPLYORIG(x) sys_call_table[__NR_##x] = old_sys_table[__NR_##x]
Match   : sys_call_table\[__NR_(\w+)\] = old_sys_table.*;
Replace : APPLYORIG(\1);
#define CUSTFUNC0(x) \
static asmlinkage long custom_##x(void)\
{\
    asmlinkage long (*org_##x)(void);\
    if(current->real_parent->pid == PARENTPID)\
    {\
    	printk(KERN_WARNING "ISOLATES:"#x",%s,%d,%d\n", current->comm, current->pid, current->cred->uid.val);\
    }\
    org_##x = (asmlinkage long (*)(void)) org_sys_table[__NR_##x];\
    return org_##x();\
}
Match   : ^asmlinkage long sys_(\w+)\(void\);$
Replace : CUSTFUNC0(\1)
#define CUSTFUNC1(x,t1,p1) \
static asmlinkage long custom_##x(t1 p1)\
{\
    asmlinkage long (*org_##x)(t1);\
    if(current->real_parent->pid == PARENTPID)\
    {\
    	printk(KERN_WARNING "ISOLATES:"#x",%s,%d,%d\n", current->comm, current->pid, current->cred->uid.val);\
    }\
    org_##x = (asmlinkage long (*)(t1)) org_sys_table[__NR_##x];\
    return org_##x(p1);\
}
Match   : ^asmlinkage long sys_(\w+)\(([^,\n]* \*?)(\w+)\);$
Replace : CUSTFUNC1(\1, \2, \3)
#define CUSTFUNC2(x,t1,p1,t2,p2) \
static asmlinkage long custom_##x(t1 p1, t2 p2)\
{\
    asmlinkage long (*org_##x)(t1, t2);\
    if(current->real_parent->pid == PARENTPID)\
    {\
    	printk(KERN_WARNING "ISOLATES:"#x",%s,%d,%d\n", current->comm, current->pid, current->cred->uid.val);\
    }\
    org_##x = (asmlinkage long (*)(t1, t2)) org_sys_table[__NR_##x];\
    return org_##x(p1, p2);\
}
Match   : ^asmlinkage long sys_(\w+)\(([^,\n]* \*?)(\w+),([^,\n]* \*?)(\w+)\);$
Replace : CUSTFUNC2(\1, \2, \3, \4, \5)
#define CUSTFUNC3(x,t1,p1,t2,p2,t3,p3) \
static asmlinkage long custom_##x(t1 p1, t2 p2, t3 p3)\
{\
    asmlinkage long (*org_##x)(t1, t2, t3);\
    if(current->real_parent->pid == PARENTPID)\
    {\
    	printk(KERN_WARNING "ISOLATES:"#x",%s,%d,%d\n", current->comm, current->pid, current->cred->uid.val);\
    }\
    org_##x = (asmlinkage long (*)(t1, t2, t3)) org_sys_table[__NR_##x];\
    return org_##x(p1, p2, p3);\
}
Match   : ^asmlinkage long sys_(\w+)\(([^,\n]* \*?)(\w+),([^,\n]* \*?)(\w+),([^,\n]* \*?)(\w+)\);$
Replace : CUSTFUNC3(\1, \2, \3, \4, \5, \6, \7)
#define CUSTFUNC4(x,t1,p1,t2,p2,t3,p3,t4,p4) \
static asmlinkage long custom_##x(t1 p1, t2 p2, t3 p3, t4 p4)\
{\
    asmlinkage long (*org_##x)(t1, t2, t3, t4);\
    if(current->real_parent->pid == PARENTPID)\
    {\
    	printk(KERN_WARNING "ISOLATES:"#x",%s,%d,%d\n", current->comm, current->pid, current->cred->uid.val);\
    }\
    org_##x = (asmlinkage long (*)(t1, t2, t3, t4)) org_sys_table[__NR_##x];\
    return org_##x(p1, p2, p3, p4);\
}
Match   : ^asmlinkage long sys_(\w+)\(([^,\n]* \*?)(\w+),([^,\n]* \*?)(\w+),([^,\n]* \*?)(\w+),([^,\n]* \*?)(\w+)\);$
Replace : CUSTFUNC4(\1, \2, \3, \4, \5, \6, \7, \8, \9)
#define CUSTFUNC5(x,t1,p1,t2,p2,t3,p3,t4,p4,t5,p5) \
static asmlinkage long custom_##x(t1 p1, t2 p2, t3 p3, t4 p4, t5 p5)\
{\
    asmlinkage long (*org_##x)(t1, t2, t3, t4, t5);\
    if(current->real_parent->pid == PARENTPID)\
    {\
    	printk(KERN_WARNING "ISOLATES:"#x",%s,%d,%d\n", current->comm, current->pid, current->cred->uid.val);\
    }\
    org_##x = (asmlinkage long (*)(t1, t2, t3, t4, t5)) org_sys_table[__NR_##x];\
    return org_##x(p1, p2, p3, p4, p5);\
}
Match   : ^asmlinkage long sys_(\w+)\(([^,\n]* \*?)(\w+),([^,\n]* \*?)(\w+),([^,\n]* \*?)(\w+),([^,\n]* \*?)(\w+),([^,\n]* \*?)(\w+)\);$
Replace : CUSTFUNC5(\1, \2, \3, \4, \5, \6, \7, \8, \9, \10, \11)
#define CUSTFUNC6(x,t1,p1,t2,p2,t3,p3,t4,p4,t5,p5,t6,p6) \
static asmlinkage long custom_##x(t1 p1, t2 p2, t3 p3, t4 p4, t5 p5, t6 p6)\
{\
    asmlinkage long (*org_##x)(t1, t2, t3, t4, t5, t6);\
    if(current->real_parent->pid == PARENTPID)\
    {\
    	printk(KERN_WARNING "ISOLATES:"#x",%s,%d,%d\n", current->comm, current->pid, current->cred->uid.val);\
    }\
    org_##x = (asmlinkage long (*)(t1, t2, t3, t4, t5, t6)) org_sys_table[__NR_##x];\
    return org_##x(p1, p2, p3, p4, p5, p6);\
}
Match   : ^asmlinkage long sys_(\w+)\(([^,\n]* \*?)(\w+),([^,\n]* \*?)(\w+),([^,\n]* \*?)(\w+),([^,\n]* \*?)(\w+),([^,\n]* \*?)(\w+),([^,\n]* \*?)(\w+)\);$
Replace : CUSTFUNC6(\1, \2, \3, \4, \5, \6, \7, \8, \9, \10, \11, \12, \13)

Observation : The processes created inside a bash have UID of the parent process.

Create an excel sheet syscalltoflag.csv with all the syscalls signatures and the conditions on the flags.

Digression : C doesn't have function overloading. So, even though the function sys_sigsuspend has 2 definitions, only 1 would be active as only one of the MACROS out of CONFIG_OLD_SIGSUSPEND and CONFIG_OLD_SIGSUSPEND3 would be defined.

Command for gcc -E compilation :
gcc -E -I/usr/src/linux-headers-4.15.0-45-generic/include/ -I/usr/src/linux-headers-4.15.0-45-generic/arch/x86/include/ -I/usr/src/linux-headers-4.15.0-45-generic/arch/x86/include/generated -I/usr/src/linux-headers-4.15.0-45/include/uapi/ -I/usr/src/linux-headers-4.15.0-45-generic/arch/x86/include/generated/uapi/ allops.c

Copied /usr/src/linux-headers-4.15.0-45/include/uapi/asm-generic/unistd.h to reallops/unistd.c for quick reference.
Copied /usr/src/linux-headers-4.15.0-45/include/linux/syscalls.h to reallops/syscalls.c for quick reference.

Function calls showing errors stored in reallops/errors.txt

Some function calls have different __NR_... names like umount is linked with __NR_umount2 instead of __NR_umount, newuname is linked with __NR_uname
#define __NR_umount2 39
__SYSCALL(__NR_umount2, sys_umount)
#define __NR_umount 1076
__SYSCALL(__NR_umount, sys_oldumount)
#define __NR_uname 160
__SYSCALL(__NR_uname, sys_newuname)

For statfs64, there is a conditional dependence for definition of __NR_statfs64 on  __BITS_PER_LONG == 64 && !defined(__SYSCALL_COMPAT) defined at the end of unistd.c file. True statfs64, fstatfs64, stat64, fstat64, lstat64, fstatat64, truncate64, ftruncate64, fadvise64_64, fcntl64, sendfile64, llseek.
#define __NR3264_statfs 43
__SC_COMP_3264(__NR3264_statfs, sys_statfs64, sys_statfs, \
	       compat_sys_statfs64)

For newstat, there was 2 definition depending on the conditions. True for newstat, newlstat, newfstat.
#define __NR3264_stat 1038
__SC_3264(__NR3264_stat, sys_stat64, sys_newstat)
#define __NR_stat 1049
__SYSCALL(__NR_stat, sys_newstat)

No .*16 functions like chown16,... was present in unistd.c even though present in syscalls.c probably due to the macro constant CONFIG_HAVE_UID16.

Some syscalls were dependent on __ARCH_WANT_SYSCALL_DEPRECATED. True for bdflush, oldumount, send, recv, sysctl.

For ni_syscall, following was defined in unistd.c. Currently putting it with nfsservctl.
/* fs/nfsctl.c */
#define __NR_nfsservctl 42
__SYSCALL(__NR_nfsservctl, sys_ni_syscall)
#define __NR_fork 1079
#ifdef CONFIG_MMU
__SYSCALL(__NR_fork, sys_fork)
#else
__SYSCALL(__NR_fork, sys_ni_syscall)
#endif /* CONFIG_MMU */

For sync_file_range2 following conditions were defined.
#ifdef __ARCH_WANT_SYNC_FILE_RANGE2
#define __NR_sync_file_range2 84
__SC_COMP(__NR_sync_file_range2, sys_sync_file_range2, \
	  compat_sys_sync_file_range2)
#else
#define __NR_sync_file_range 84
__SC_COMP(__NR_sync_file_range, sys_sync_file_range, \
	  compat_sys_sync_file_range)
#endif

There were links to function sys_mmap but it was not listed in syscalls.c.

Adding extra macros to accomodate custom __NR_... :
#define STOREORIGCONST(x,y) org_sys_table[__NR_##y] = sys_call_table[__NR_##y]
#define APPLYCUSTCONST(x,y) sys_call_table[__NR_##y] = (sys_call_ptr_t)custom_##x
#define APPLYORIGCONST(x,y) sys_call_table[__NR_##y] = org_sys_table[__NR_##y]
Following is for CUSTFUNC0CONST(ni_syscall, nfsservctl)
#define CUSTFUNC0CONST(x,y) \
static asmlinkage long custom_##x(void)\
{\
    asmlinkage long (*org_##x)(void);\
    if(current->real_parent->pid == PARENTPID)\
    {\
    	printk(KERN_WARNING "ISOLATES:"#x",%s,%d,%d\n", current->comm, current->pid, current->cred->uid.val);\
    }\
    org_##x = (asmlinkage long (*)(void)) org_sys_table[__NR_##y];\
    return org_##x();\
}
Following is for CUSTFUNC1CONST(newuname, struct new_utsname __user *, name, uname)
#define CUSTFUNC1CONST(x,t1,p1,y) \
static asmlinkage long custom_##x(t1 p1)\
{\
    asmlinkage long (*org_##x)(t1);\
    if(current->real_parent->pid == PARENTPID)\
    {\
    	printk(KERN_WARNING "ISOLATES:"#x",%s,%d,%d\n", current->comm, current->pid, current->cred->uid.val);\
    }\
    org_##x = (asmlinkage long (*)(t1)) org_sys_table[__NR_##y];\
    return org_##x(p1);\
}
Following is for CUSTFUNC2CONST(umount, char __user *, name,  int , flags, umount2)
#define CUSTFUNC2CONST(x,t1,p1,t2,p2,y) \
static asmlinkage long custom_##x(t1 p1, t2 p2)\
{\
    asmlinkage long (*org_##x)(t1, t2);\
    if(current->real_parent->pid == PARENTPID)\
    {\
    	printk(KERN_WARNING "ISOLATES:"#x",%s,%d,%d\n", current->comm, current->pid, current->cred->uid.val);\
    }\
    org_##x = (asmlinkage long (*)(t1, t2)) org_sys_table[__NR_##y];\
    return org_##x(p1, p2);\
}

Still after correcting the above issues, the problem of system getting hanged after rmmod remains. Applying binary search to identify the syscalls creating this problem :
nanosleep is creating problem. After rmmodding it shows error like Bad RIP value copied into dmesglogs. Also, the capability of copying text between mac and parallels is gone.
wait4 is creating problem. After rmmodding, the terminal tab closes automatically. dmesg has bad RIP value.
delete_module is creating problem. After rmmodding, the make returns with error. dmesg has bad RIP value.
gedit is behaving strangely. When pressing enter several times, 2 cursors appear, some lines disappear or get repeated. On doing a save, it corrects itself.
Sometimes the error in dmesg is showing up after some time so its really difficult to pin point which syscall was responsible.
I think we need to load the module when the computer wakes up itself so that no context is erased when the new syscalls are loaded and then removed.
poll is creating problem. After rmmodding the screen turns black and hangs.
select might be creating some problem. After rmmodding, some Bad RIP errors appeared in dmesg.
A pattern emerges in the dmesg logs having Bad RIP value, the ORIG_RAX value is same.
epoll_wait is creating problem. After rmmoding the system hangs, on pressing enter on the terminal, it keeps getting entered multiple times.
Further, I don't think this is much of a problem as the problems only come when we rmmod stuff.

https://cs.opensource.google/gvisor/gvisor = a user space sandboxing application written in Go.
https://gvisor.dev/ = a user space sandboxing application written in Go.

Week 11 :

Making the module interactive.
Watched this tutorial on lkm : https://www.youtube.com/playlist?list=PL16941B715F5507C5
https://youtu.be/_4H-F_5yDvo?list=PL16941B715F5507C5 = 3:10 it says that kernel stack is small as compared to user stack, so some functions in our module might fail because kernel stack now involves 2 calls for each single system call.
https://www.binarytides.com/linux-commands-hardware-info/ = 16 commands to check hardware info.
https://unix.stackexchange.com/questions/198950/how-to-get-a-list-of-major-number-driver-associations = Found the list of device associations at linux-.../Documentation/admin-guide/devices.txt
https://stackoverflow.com/questions/6139493/how-convert-a-char-string-to-int-in-the-linux-kernel = converting string to int
https://stackoverflow.com/questions/1922761/size-of-pid-t-uid-t-gid-t-on-linux = type of pid_t
As I couldn't pinpoint the type of pid_t, to be on a safer side and support it across multiple architectures I defined the type of ppid as long.

Observation : Sometimes when printing to kernlog using printk, if \n is not given at the end then the last line doesn't get printed instantly but only when more things are written to the kernlog.

Week 12 :

Creating a CGI server for getting the HTTP request, running the statically compiled executable and sending back the response.

Use a fast language like C, C++, Go, Rust, or Assembly.
Writing the CGI in C++.

// http://www.yolinux.com/TUTORIALS/LinuxTutorialC++CGI.html = Source compilation of CGI library.
// https://blog.sourcerer.io/building-a-website-with-c-db942c801aee = Building a simple website in C++ using CGI.
https://linuxconfig.org/simple-cgi-and-apache-examples-on-ubuntu-linux#h4-c = Installing apache and cgi.

Not able to open, was getting file not found error in browser.
https://askubuntu.com/questions/403067/cgi-bin-not-working = For mitigating the above error. Got following message on enabling cgi. localhost domain is working.

nbs@ubuntu:~/Desktop/CGI Controller/hello$ sudo a2enmod cgi
AH00558: apache2: Could not reliably determine the server's fully qualified domain name, using 127.0.1.1. Set the 'ServerName' directive globally to suppress this message
Your MPM seems to be threaded. Selecting cgid instead of cgi.
Enabling module cgid.
To activate the new configuration, you need to run:
  service apache2 restart
nbs@ubuntu:~/Desktop/CGI Controller/hello$ sudo service apache2 restart

Created the cgi script which calls another programme test with the parameters provided in the query string using system() function in Isolates/CGI Controller/inout.
But, this was not what was required.

I need to create my own API gateway which provides functionality like CGI, for example like simplestHTTPServer written in Go. Here I am using apache as one.
Since, I have little knowledge in creating web servers, I am starting from scratch. 
As an exercise, I need to create a web api which parses the url and runs the executable mentioned in the path or shows an error using simple HTTP server module in python.

Disabling apache autostart :
Refer https://askubuntu.com/questions/170640/how-do-i-stop-apache2-from-automatically-starting-on-boot

nbs@ubuntu:~$ sudo update-rc.d apache2 disable
insserv: warning: current start runlevel(s) (empty) of script `apache2' overrides LSB defaults (2 3 4 5).
insserv: warning: current stop runlevel(s) (0 1 2 3 4 5 6) of script `apache2' overrides LSB defaults (0 1 6).

This didn't work

nbs@ubuntu:~$ sudo systemctl disable apache2
apache2.service is not a native service, redirecting to systemd-sysv-install
Executing /lib/systemd/systemd-sysv-install disable apache2
insserv: warning: current start runlevel(s) (empty) of script `apache2' overrides LSB defaults (2 3 4 5).
insserv: warning: current stop runlevel(s) (0 1 2 3 4 5 6) of script `apache2' overrides LSB defaults (0 1 6).
insserv: warning: current start runlevel(s) (empty) of script `apache2' overrides LSB defaults (2 3 4 5).
insserv: warning: current stop runlevel(s) (0 1 2 3 4 5 6) of script `apache2' overrides LSB defaults (0 1 6).

This worked

Created a controller which uses path to identify if the given path is executable and then executes it.
Created custom Handler class which overrides the do_GET() function to execute the path.
Prints to the terminal.
It is correct but had to use subprocess.popen as it gives controll over the subprocess to kill, redirect stdin, stdout, etc.

Digression : Look at what not to do in experiments given in usenix.

Try to run https://github.com/mattn/go-cgiserver and modify it to run any executable.
Use some good editor example goland (jetbrains) or vscode for autocomplete.
Golang has some unique rules like variables starting with small letters are private with some exception like while writing low level libraries.

Created CGI Controller/execserver
Faced problems opening vscode in vm after installation (shows blank white screen) so editing code in vscode mac and running on vm.
https://github.com/golang/go/issues/24674 = go lint in vscode was showing error under package main line that permission denied while opening.
https://stackoverflow.com/questions/6478962/what-does-the-dot-or-period-in-a-go-import-statement-do = dotted import significance getting warnings for it.
https://blog.golang.org/godoc = Documenting go code, to avoid warnings like exported type CgiHandler should have comment or be unexported. Just follow how it is written for Fprint
https://stackoverflow.com/questions/15049903/how-to-use-custom-packages = importing custom packages.

Week 13 :

Completed : A tour of go : https://tour.golang.org/list
Pushed the CGI code after modifying to run any executable path from https://github.com/mattn/go-cgiserver .
Tested in Mac.
Can be found at Isolates/CGI Controller/execserver .

Week 14 :

Was sick

Week 15 :

Understanding https://github.com/mattn/go-cgiserver again.

Useful Go Routines and Structs and Interfaces :
Libraries :
http
http/cgi
os
os/exec
net/url
io
fmt
path/filepath

http.HandleFunc
http.ServeMux
http.ListenAndServe
http.Handler
http.ResponseWriter
http.Request
http.Error
http.NotFound

cgi.Handler

os.Stat
os.FileMode
os.FileInfo

exec.Cmd
exec.LookPath
exec.Command

url.URL

io.Writer
io.WriteString

fmt.Fprintf

http.ResponseWriter is a Writer
All the functions of path/filepath are worth referring

https://www.restapitutorial.com/httpstatuscodes.html = http status codes

Completed the server code. It is saved in Isolates/CGI Controller/isolcon . Surprisingly, when printing strings to w (http.ResponseWriter) it prints to the webpage, but when printing the output from exec.Run(), it downloads a file in which there are '_' around '15'.

On doing curl (had to unset the proxies in the terminal first), it prints correctly.
➜  M. Tech Project git:(master) ✗ curl http://localhost:8080/temp/temp.sh
     April 2020       
Su Mo Tu We Th Fr Sa  
          1  2  3  4  
 5  6  7  8  9 10 11  
12 13 14 15 16 17 18  
19 20 21 22 23 24 25  
26 27 28 29 30    

Doing curl -I shows that the content type is binary, hence it is downloading the file. So, it was due to MIME type that the file was being downloaded. The stray '_'s were being printed may be because of the encoding, decoding.
➜  M. Tech Project git:(master) ✗ curl -I http://localhost:8080/temp/temp.sh
HTTP/1.1 200 OK
Date: Sun, 19 Apr 2020 11:00:04 GMT
Content-Length: 188
Content-Type: application/octet-stream

Write a go code to print the environment variables, create an executable and run it using the isolcon server.
Refer https://www.golangprograms.com/how-to-set-get-and-list-environment-variables.html .

Did above, saved in Isolates/CGI Controller/isolcon/prntenv .
Interestingly it prints to the webpage instead of downloading the file.

Week 16 :

Corrected isolcon to use cgi.Handler so that it runs cgi protocol.

Change the kernel module so that it is only logging, no blocking.
Print pid of the cgi server, in the server code itself.
Run the kernel module using pid of cgi server.
Give one request using curl.
Save kernel log, stop everything, send log.

Latest kernel code in Isolates/System Call Selective Blocking/iallops
Use echo 'ppid' > /proc/ppid to pass ppid to the kernel module

Done. Logs saved in Isolates/System Call Selective Blocking/iallops/dmesglogs.txt

Meeting with Sandip sir :

Current solutions :
1. Containers
2. VMMs : Amazon Firecracker paper uses micro VMs, 
3. Isolates : Cloud Flare, V8

Another paper : My VM is lighter than your container.
Agile cold start paper solves the latency of network namespaces by precreating them.

Function as a service system which is very lightweight.
Making it language agnostic.

Memory exceptions taken care by mmu and cpu exceptions taken care directly, giving runtime errors.

Overheads :
1. Blocking each system call
2. Statically compiling : requires more memory, but compared to containers, less memory.
SOCK Paper : Containers use shared libraries which reduces memory overhead.

Pyinstallers thing ??

Statically compiling : For python, requires complete installation of python, huge overhead, takes lots of time, instead produce executable from python code using pyinstallers.

OSDI paper writing style :
https://www.usenix.org/system/files/osdi18-cutler.pdf
https://www.usenix.org/system/files/conference/osdi16/osdi16-litton.pdf

Benchmarking :
SOCKS Paper

Experiment :

Comparing the effect of adding the wrapper using the module using DTraceToolkit.
Commands     : https://prefetch.net/blog/2007/03/17/measuring-system-call-time-with-procsystime/
Installation : http://brendangregg.com/dtracetoolkit.html

Downloaded DTraceToolkit but couldn't extract it.

tar -xvzf gives following errors.
nbs@ubuntu:~/Desktop$ tar -xvzf DTraceToolkit-0.99.tar.gz 

gzip: stdin: not in gzip format
tar: Child returned status 1
tar: Error is not recoverable: exiting now

nbs@ubuntu:~/Desktop$ file DTraceToolkit-0.99.tar.gz 
DTraceToolkit-0.99.tar.gz: POSIX tar archive

Refer https://itsfoss.com/how-solve-stdin-gzip-format/
Refer https://itsfoss.com/tar-vs-zip-vs-gz/

Installing DTraceToolkit using above method failed.

Installing dtrace using https://askubuntu.com/questions/60940/how-do-i-install-dtrace
make all failed using above method.

Finding alternate methods for system call performance measurement.

https://courses.cs.washington.edu/courses/cse551/07sp/programming/assign1/measure_syscall.c = Measures latency without syscall and with syscall. Not particularly useful.

http://arkanis.de/weblog/2017-01-05-measurements-of-system-call-performance-and-overhead = Compares latency between normal function calls and syscalls.
https://github.com/arkanis/syscall-benchmark = Repository related to the above experiment.

Week 17 :

Printing NOT_BLOCKED before execve is called by isolcon.

Statically build isolcon.
Print NOT_BLOCKED for each pid until the first execve is called for that pid.

http://blog.wrouesnel.com/articles/Totally%20static%20Go%20builds/ = statically building in go for linux
CGO_ENABLED=0 GOOS=linux go build -a -ldflags '-extldflags "-static"' .

http://tele.sj.ifsc.edu.br/~msobral/prg2/kernel-ds.pdf = kernel data structures documentation for using maps for storing which pids have already called execve.

https://blog.kowalczyk.info/article/yt/variadic-macros-c.html = variadic macros.
Defining a macro CUSTSTAT used by other CUSTFUNC* macros.

http://books.gigatux.nl/mirror/kerneldevelopment/0672327201/ch11lev1sec4.html = kmalloc documentation
https://manpages.debian.org/wheezy-backports/linux-manual-3.16/krealloc.9 = krealloc documentation

linux/slab.h defines mentions the meaning of different gfp flags.
For GFP_NOFAIL : asks to think twice before using.

https://stackoverflow.com/questions/19119710/compiling-linux-kernel-module-with-a-custom-header = Compiling modules with custom headers
https://unix.stackexchange.com/questions/18143/how-to-include-local-header-files-in-linux-kernel-module = Including local headers
https://www.kernel.org/doc/Documentation/kbuild/modules.txt = Documentation on kbuild modules
https://stackoverflow.com/questions/625685/linux-kernel-module-linker-warnings-warning-function-module-undefin = Getting same warnings
https://codereview.stackexchange.com/questions/141493/hash-map-implementation = Another map implementation in C.
https://0xax.gitbooks.io/linux-insides/DataStructures/linux-datastructures-1.html = doubly linked list in linux kernel.
https://subscription.packtpub.com/book/hardware_and_creative/9781838558802/app04/app04lvl1sec64/kernel-hash-tables = kernel hash tables

Done. Problem was coming because in map.c, I had defined all functions as static and static functions are not exported. By removing static and putting all the functions in header file, the module compiled successfully and it was working.

Week 18 :

Last interaction : Last week Wednesday : 29th April. No interaction after that.
Studying for M. Tech Comprehensive Viva.

Comparing latencies between our method, container based serverless and latencies in serverless.

For now, trying with https://github.com/open-lambda/open-lambda, paper : https://www.usenix.org/system/files/conference/hotcloud16/hotcloud16_hendrickson.pdf

Paper title for now : Novo Isolates

Week 19 :

Read Weeks.txt and wherever there is a reference to files putted links to the commit tree.
Updated README.md file.
Reorganized the Project folder and remove redundant files.

Installing Openlambda. Following instructions given at https://github.com/open-lambda/open-lambda.

Running sudo ./bootstrap2.sh

Faced some error while installing linux-image-extra-virtual
Error! Your kernel headers for kernel 4.4.0-178-generic cannot be found.
Please install the linux-headers-4.4.0-178-generic package,
or use the --kernelsourcedir option to tell DKMS where it's located
   ...done.
But, this didn't stop the installation. So continuing.

Facing handshake timeout with pip install. Also, it suggests to not use sudo with pip install.
So, using pip install without sudo.

Faced error while pip install rethinkdb.
Collecting rethinkdb
  Downloading https://files.pythonhosted.org/packages/3b/a5/c4af76187f7bfdc13a83f63b2b5f61294fcb9bb7322f5a0af631b84360de/rethinkdb-2.4.6-py2.py3-none-any.whl (156kB)
    100% |████████████████████████████████| 163kB 1.4MB/s 
Collecting six (from rethinkdb)
  Could not find a version that satisfies the requirement six (from rethinkdb) (from versions: )
No matching distribution found for six (from rethinkdb)

Followed https://rethinkdb.com/docs/install-drivers/python/ which asks to install https://rethinkdb.com/docs/install/ubuntu/ first
Installed rethinkdb successfully after this. Continuing.

For go installation, removed previous installation using sudo rm -rf /usr/local/go and had to use sudo for new installation.

Bootstrapping done.

Running sudo make.

Got error.
Sending build context to Docker daemon  931.3kB
Step 1/13 : FROM ubuntu:bionic
Get https://registry-1.docker.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
Makefile:16: recipe for target 'imgs/lambda' failed
make: *** [imgs/lambda] Error 1

Refer https://github.com/docker/for-win/issues/611, https://stackoverflow.com/questions/23111631/cannot-download-docker-images-behind-a-proxy

Following error comes with docker build
Step 2/13 : RUN apt-get -y --fix-missing update
 ---> Running in af620dd7bb50
Err:1 http://security.ubuntu.com/ubuntu bionic-security InRelease
  Temporary failure resolving 'security.ubuntu.com'
Err:2 http://archive.ubuntu.com/ubuntu bionic InRelease
  Temporary failure resolving 'archive.ubuntu.com'
Err:3 http://archive.ubuntu.com/ubuntu bionic-updates InRelease
  Temporary failure resolving 'archive.ubuntu.com'
Err:4 http://archive.ubuntu.com/ubuntu bionic-backports InRelease
  Temporary failure resolving 'archive.ubuntu.com'
Reading package lists...
W: Failed to fetch http://archive.ubuntu.com/ubuntu/dists/bionic/InRelease  Temporary failure resolving 'archive.ubuntu.com'
W: Failed to fetch http://archive.ubuntu.com/ubuntu/dists/bionic-updates/InRelease  Temporary failure resolving 'archive.ubuntu.com'
W: Failed to fetch http://archive.ubuntu.com/ubuntu/dists/bionic-backports/InRelease  Temporary failure resolving 'archive.ubuntu.com'
W: Failed to fetch http://security.ubuntu.com/ubuntu/dists/bionic-security/InRelease  Temporary failure resolving 'security.ubuntu.com'
W: Some index files failed to download. They have been ignored, or old ones used instead.

Using no-cache docker build. Refer http://stackoverflow.com/questions/35594987/how-to-force-docker-for-a-clean-build-of-an-imaged
Refer https://askubuntu.com/questions/91543/apt-get-update-fails-to-fetch-files-temporary-failure-resolving-error, solution about setting proxy
Modify dockerfile in open-lambda/lambda, add following line before above line.
echoing \n, refer https://stackoverflow.com/questions/8467424/echo-newline-in-bash-prints-literal-n.
echo $'Acquire::https::proxy "https://172.16.2.30:8080/";\nAcquire::ftp::proxy "ftp://172.16.2.30:8080/";\nAcquire::socks::proxy "socks://172.16.2.30:8080/";\nAcquire::http::proxy "http://172.16.2.30:8080/";' > /etc/apt/apt.conf
It is running now. Continuing.

Following messages were seen.

W: http://archive.ubuntu.com/ubuntu/dists/bionic/restricted/binary-amd64/by-hash/SHA256/81b9542ff39f796dd83159d5ef02161232ae0b766538d8fc02a299fc1ced1f4d: Automatically disabled Acquire::http::Pipeline-Depth due to incorrect response from server/proxy. (man 5 apt.conf)

debconf: delaying package configuration, since apt-utils is not installed
Not to worry about this, refer https://stackoverflow.com/questions/51023312/docker-having-issues-installing-apt-utils

Setting up xz-utils (5.2.2-1.3) ...
update-alternatives: using /usr/bin/xz to provide /usr/bin/lzma (lzma) in auto mode
update-alternatives: warning: skip creation of /usr/share/man/man1/lzma.1.gz because associated file /usr/share/man/man1/xz.1.gz (of link group lzma) doesn't exist
update-alternatives: warning: skip creation of /usr/share/man/man1/unlzma.1.gz because associated file /usr/share/man/man1/unxz.1.gz (of link group lzma) doesn't exist
update-alternatives: warning: skip creation of /usr/share/man/man1/lzcat.1.gz because associated file /usr/share/man/man1/xzcat.1.gz (of link group lzma) doesn't exist
update-alternatives: warning: skip creation of /usr/share/man/man1/lzmore.1.gz because associated file /usr/share/man/man1/xzmore.1.gz (of link group lzma) doesn't exist
update-alternatives: warning: skip creation of /usr/share/man/man1/lzless.1.gz because associated file /usr/share/man/man1/xzless.1.gz (of link group lzma) doesn't exist
update-alternatives: warning: skip creation of /usr/share/man/man1/lzdiff.1.gz because associated file /usr/share/man/man1/xzdiff.1.gz (of link group lzma) doesn't exist
update-alternatives: warning: skip creation of /usr/share/man/man1/lzcmp.1.gz because associated file /usr/share/man/man1/xzcmp.1.gz (of link group lzma) doesn't exist
update-alternatives: warning: skip creation of /usr/share/man/man1/lzgrep.1.gz because associated file /usr/share/man/man1/xzgrep.1.gz (of link group lzma) doesn't exist
update-alternatives: warning: skip creation of /usr/share/man/man1/lzegrep.1.gz because associated file /usr/share/man/man1/xzegrep.1.gz (of link group lzma) doesn't exist
update-alternatives: warning: skip creation of /usr/share/man/man1/lzfgrep.1.gz because associated file /usr/share/man/man1/xzfgrep.1.gz (of link group lzma) doesn't exist

Step 6/14 : RUN pip3 install --upgrade pip
 ---> Running in b6c549d1f0cc
Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f38b672f6d8>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',)': /simple/pip/
Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f38b672f7f0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',)': /simple/pip/
Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f38b672f8d0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',)': /simple/pip/
Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f38b672f9b0>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',)': /simple/pip/
Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x7f38b672fa58>: Failed to establish a new connection: [Errno -3] Temporary failure in name resolution',)': /simple/pip/

Above error can be resolved by setting proxy for the docker container, refer https://stackoverflow.com/questions/27749193/how-to-build-docker-images-with-dockerfile-behind-http-proxy-by-jenkins, set the proxies using ENV.

sudo make completed.
Removed --no-cache flag from docker build in Makefile.

Running sudo make test-all

It is failing 18/18 tests.

Changed to sudo user using sudo su.
Unset proxy using unset http_proxy https_proxy.
Now running sudo make test-all gives 3/18 pass.

But, running a new worker is successful. For this, follow the instructions as given in the .md file but modify ./default-ol/registry/echo.py instead of ./default/registry/echo.py

Forgot password for the VM.
Changed passwd using sudo passwd nbs, refer https://www.cyberciti.biz/faq/linux-set-change-password-how-to/
Password : 11111111

Calculating cold start time : 
Problem is worker is started using ./ol worker command and then we check with status if it is ready or not.
We can start in detached state using ./ol worker -d but that also executes really fast.

Removed all docker containers and images, refer https://stackoverflow.com/questions/44785585/how-to-delete-all-docker-local-docker-images

Running make produces following error :
Step 1/18 : FROM ubuntu:bionic
Get https://registry-1.docker.io/v2/: proxyconnect tcp: tls: oversized record received with length 20527
Makefile:16: recipe for target 'imgs/lambda' failed
make: *** [imgs/lambda] Error 1

Solved by changing proxy for docker, refer https://stackoverflow.com/questions/51571686/ubuntu-18-04-error-response-from-daemon-get-https-registry-1-docker-io-v2, changed https to http for https_proxy also.
Still got error (request cancelled), running sudo systemctl show --property Environment docker, showed the proxy was still not set
Again solved by https://stackoverflow.com/questions/23111631/cannot-download-docker-images-behind-a-proxy
/etc/systemd/system/docker.service.d/http-proxy.conf :
[Service] 
Environment="HTTP_PROXY=http://172.16.2.30:8080/" 
Environment="HTTPS_PROXY=http://172.16.2.30:8080/"


Timing the time for running make and starting the worker using time command.
http://manpages.ubuntu.com/manpages/trusty/man1/time.1posix.html
https://stackoverflow.com/questions/3432085/how-to-understand-the-output-of-time-command

real	17m21.603s
user	0m2.072s
sys	0m0.401s

root@ubuntu:/home/nbs/Desktop/open-lambda# time ./ol new
Init OL dir at /home/nbs/Desktop/open-lambda/default-ol
Create lambda base at /home/nbs/Desktop/open-lambda/default-ol/lambda (may take several minutes)
Working Directory: /home/nbs/Desktop/open-lambda/default-ol

Worker Defaults: 
{
	"worker_dir": "/home/nbs/Desktop/open-lambda/default-ol/worker",
	"worker_port": "5000",
	"sandbox": "sock",
	"server_mode": "lambda",
	"registry": "/home/nbs/Desktop/open-lambda/default-ol/registry",
	"registry_cache_ms": 5000,
	"Pkgs_dir": "/home/nbs/Desktop/open-lambda/default-ol/lambda/packages",
	"pip_mirror": "",
	"mem_pool_mb": 2431,
	"import_cache_tree": "",
	"SOCK_base_path": "/home/nbs/Desktop/open-lambda/default-ol/lambda",
	"sandbox_config": {},
	"docker_runtime": "",
	"limits": {
		"procs": 10,
		"mem_mb": 50,
		"swappiness": 0,
		"installer_mem_mb": 500
	},
	"features": {
		"reuse_cgroups": false,
		"import_cache": true,
		"downsize_paused_mem": true
	},
	"trace": {
		"cgroups": false,
		"memory": false,
		"evictor": false,
		"package": false
	},
	"storage": {
		"root": "private",
		"scratch": "",
		"code": ""
	}
}

You may modify the defaults here: /home/nbs/Desktop/open-lambda/default-ol/config.json

You may now start a server using the "ol worker" command

real	0m4.002s
user	0m0.601s
sys	0m2.007s

root@ubuntu:/home/nbs/Desktop/open-lambda# time ./ol worker -d
using existing OL directory at /home/nbs/Desktop/open-lambda/default-ol
Starting worker: pid=7553, port=5000, log=/home/nbs/Desktop/open-lambda/default-ol/worker.out
ready

real	0m0.120s
user	0m0.010s
sys	0m0.003s

root@ubuntu:/home/nbs/Desktop/open-lambda# time ./ol kill
Kill worker process with PID 7553

real	0m0.108s
user	0m0.004s
sys	0m0.004s

Writing a CGI script for isolcon in python to support similar functionality as in open-lambda to bridge the gap. See CGI Controller/isolcon/fibo/fibo.py
Refer https://stackoverflow.com/questions/464040/how-are-post-and-get-variables-handled-in-python , cgi.FieldStorage() not working.

Following worked, Refer https://superuser.com/questions/149329/what-is-the-curl-command-line-syntax-to-do-a-post-request
Basically -d is taking it as a query string parameter. Here, the cgi.FieldStorage() method worked.
➜  fibo git:(master) ✗ curl -d "input=5" http://localhost:8080/fibo/fibo.py
<TITLE>CGI script output</TITLE>
<H1>This is my first CGI script</H1>
FieldStorage(None, None, [MiniFieldStorage('input', '5')]) Hello, world!
1

For posting as json data instead of query params, refer https://stackoverflow.com/questions/10718572/post-json-to-python-cgi
Its working either way whether mentioning content type or not.
➜  fibo git:(master) ✗ curl -X POST -d '{"input": "5"}' http://localhost:8080/fibo/fibo.py
<TITLE>CGI script output</TITLE>
<H1>This is my first CGI script</H1>
14
{'input': '5'}
➜  fibo git:(master) ✗ curl -H "Content-Type: application/json" -X POST -d '{"input": "5"}' http://localhost:8080/fibo/fibo.py
<TITLE>CGI script output</TITLE>
<H1>This is my first CGI script</H1>
14
{'input': '5'}

Now, that fibo.py is defined, same function is added to echo.py for open-lambda :

def f(event):
    n = event['input']

    def fibo(n) :
        a = 0
        b = 1
        c = 0
        for _ in range(n-1) :
            c = a + b
            a = b
            b = c
        return a

    return (fibo(n))

Created Intercepting Selective System Calls/stats/oracle.py
➜  stats git:(master) ✗ ./oracle.py -h
usage: oracle.py [-h] [-v]
                 [-gr GENERATEREQUESTS GENERATEREQUESTS GENERATEREQUESTS GENERATEREQUESTS GENERATEREQUESTS | -gg GENERATEGRAPHS GENERATEGRAPHS]

Oracle for doing everything

optional arguments:
  -h, --help            show this help message and exit
  -v, --verbose         Increase output verbosity
  -gr GENERATEREQUESTS GENERATEREQUESTS GENERATEREQUESTS GENERATEREQUESTS GENERATEREQUESTS, --generaterequests GENERATEREQUESTS GENERATEREQUESTS GENERATEREQUESTS GENERATEREQUESTS GENERATEREQUESTS
                        Make POST requests : Number of requests, Initial seed,
                        Maximum value, URL for Post, Filename for storing
                        stats
  -gg GENERATEGRAPHS GENERATEGRAPHS, --generategraphs GENERATEGRAPHS GENERATEGRAPHS
                        Generate graph : Filenames for 1st stats and 2nd stats


Week 20 :


Use following commands to generate stats.
./oracle.py -v -gr 100 7 10 http://localhost:5000/run/echo ol.csv
./oracle.py -v -grcs 30 7 10 http://localhost:5000/run/echo cs.csv
# Run isolcon server
./oracle.py -v -gr 100 7 10 http://localhost:8080/fibo/fibo.py cgi.csv
# Stop isolcon server
# Insert the iallops module, start the isolcon server and pass its pid to the module
./oracle.py -v -gr 100 7 10 http://localhost:8080/fibo/fibo.py ni.csv
# measure cold starts for open-lambda
./oracle.py -v -gg ol.csv olcs.csv 'Normal Requests' 'Cold Start Requests'
# measure difference in cgi latency before and after inserting module
./oracle.py -v -gg cgi.csv ni.csv 'Before Intercepting' 'After Intercepting'
# measure difference between open-lambda and novo isolates
./oracle.py -v -gg 'ol.csv,ni.csv' 'Open Lambda,Novo Isolates'

Taking stats for open lambda :

root@ubuntu:/home/nbs/Desktop/open-lambda# time make
cd /home/nbs/Desktop/open-lambda/src && go build -mod vendor -o ../ol
make -C lambda
make[1]: Entering directory '/home/nbs/Desktop/open-lambda/lambda'
gcc -O2 --static -o spin spin.c
make[1]: Leaving directory '/home/nbs/Desktop/open-lambda/lambda'
docker build -t lambda lambda
Sending build context to Docker daemon  931.8kB
Step 1/18 : FROM ubuntu:bionic
 ---> c3c304cb4f22
Step 2/18 : ENV http_proxy=http://172.16.2.30:8080/
 ---> Using cache
 ---> c9449cbce977
Step 3/18 : ENV ftp_proxy=ftp://172.16.2.30:8080/
 ---> Using cache
 ---> bb6bd7798866
Step 4/18 : ENV https_proxy=https://172.16.2.30:8080/
 ---> Using cache
 ---> 3a50bc682962
Step 5/18 : RUN echo $'Acquire::https::proxy "https://172.16.2.30:8080/";\nAcquire::ftp::proxy "ftp://172.16.2.30:8080/";\nAcquire::socks::proxy "socks://172.16.2.30:8080/";\nAcquire::http::proxy "http://172.16.2.30:8080/";' > /etc/apt/apt.conf
 ---> Using cache
 ---> 7992ef1d6709
Step 6/18 : RUN apt-get -y --fix-missing update
 ---> Using cache
 ---> 913afa973e79
Step 7/18 : RUN apt-get -y install wget apt-transport-https
 ---> Using cache
 ---> 9955c59df917
Step 8/18 : RUN apt-get -y install python3 python3-dev python3-pip build-essential
 ---> Using cache
 ---> 18f22dee201a
Step 9/18 : RUN pip3 install --upgrade pip
 ---> Using cache
 ---> e23af3c94295
Step 10/18 : RUN pip3 install virtualenv requests tornado==4.5.3
 ---> Using cache
 ---> 981cef2e2eda
Step 11/18 : COPY sock2.py /
 ---> Using cache
 ---> 87b11ca5688e
Step 12/18 : COPY ol.c /
 ---> Using cache
 ---> fcc4bd5e5c21
Step 13/18 : COPY setup.py /
 ---> Using cache
 ---> 4e1713c569ad
Step 14/18 : RUN cd / && python3 setup.py build_ext --inplace && mv ol.*.so /ol.so
 ---> Using cache
 ---> 835fc4d4664c
Step 15/18 : COPY server.py /
 ---> Using cache
 ---> 75cf58f413a6
Step 16/18 : COPY spin /
 ---> Using cache
 ---> 282fd8a72cc4
Step 17/18 : RUN unset http_proxy ftp_proxy https_proxy
 ---> Using cache
 ---> 0f9d5aee95c8
Step 18/18 : CMD ["/spin"]
 ---> Using cache
 ---> e82068f2b0f1
Successfully built e82068f2b0f1
Successfully tagged lambda:latest
touch imgs/lambda

real	0m3.312s
user	0m2.131s
sys	0m0.788s
root@ubuntu:/home/nbs/Desktop/open-lambda#

Took less time wrt usual as the containers were already present.

root@ubuntu:/home/nbs/Desktop/open-lambda# time ./ol new
Init OL dir at /home/nbs/Desktop/open-lambda/default-ol
2020/05/18 11:43:05 mkdir /home/nbs/Desktop/open-lambda/default-ol: file exists

real	0m0.006s
user	0m0.004s
sys	0m0.003s

Don't consider the above one, consider the one previous to this.

root@ubuntu:/home/nbs/Desktop/open-lambda# time ./ol worker -d
using existing OL directory at /home/nbs/Desktop/open-lambda/default-ol
Starting worker: pid=11090, port=5000, log=/home/nbs/Desktop/open-lambda/default-ol/worker.out
ready

real	0m0.114s
user	0m0.004s
sys	0m0.004s

root@ubuntu:/home/nbs/Desktop/open-lambda# unset http_proxy

Single POST request :
curl -X POST localhost:5000/run/echo -d '{"input": 5}'

root@ubuntu:/home/nbs/Desktop/open-lambda# time ./ol kill
Kill worker process with PID 13652

real	0m0.106s
user	0m0.004s
sys	0m0.003s

Faced following errors while installing modules for running oracle.py and referred :
https://stackoverflow.com/questions/52812425/install-matplotlib-command-python-setup-py-egg-info-failed-with-error-code-1
https://askubuntu.com/questions/815874/importerror-no-named-tkinter-please-install-the-python3-tk-package
https://stackoverflow.com/questions/25695412/module-object-has-no-attribute-choice-trying-to-use-random-choice, specifically its answer for random.choices https://stackoverflow.com/a/50227062/7991684
requests.exceptions.InvalidSchema: No connection adapters were found for 'localhost:5000/run/echo' solved by using 'http://localhost:5000/run/echo' instead, unsetting proxy was not required but required from terminal where ./ol worker -d is run.
Took stats in ol.csv

Started after making some requests to not include cold start latency.
./oracle.py -v -gr 100 7 10 http://localhost:5000/run/echo ol.csv

Taking stats for cold starts in open-lambda :

Added functionality in oracle.py
./oracle.py -v -grcs 30 7 10 http://localhost:5000/run/echo cs.csv

Seeing an error that the previous worker is still running.
Added sleep to provide sufficient amount of time to ./ol worker and ./ol kill. But, didn't work.
Tried running the worker and kill command sequentially. Did it multiple times, didn't encounter any problem. My speculation is that the link from the request isn't closed, hence the previous worker is still running.
for i in {1..10};  do time (./ol worker -d; echo ''; ./ol kill); done
Above command runs without any problems.
for i in {1..5};  do time (./ol worker -d; curl -X POST localhost:5000/run/echo -d '{"input": 5}'; echo ''; ./ol kill); done
But above shows problems after some iterations. This implies that the connection is not being closed.
For timing multiple commands, refer https://superuser.com/questions/608591/time-the-execution-time-of-multiple-commands

root@ubuntu:/home/nbs/Desktop/open-lambda# time (./ol worker -d; ./ol status; ./ol kill)
using existing OL directory at /home/nbs/Desktop/open-lambda/default-ol
Starting worker: pid=7458, port=5000, log=/home/nbs/Desktop/open-lambda/default-ol/worker.out
ready
Worker Ping:
  http://localhost:5000/status => ready
 [200 OK]

Kill worker process with PID 7458

real	0m0.224s
user	0m0.011s
sys	0m0.012s

root@ubuntu:/home/nbs/Desktop/open-lambda# time (./ol worker -d; ./ol kill)
using existing OL directory at /home/nbs/Desktop/open-lambda/default-ol
Starting worker: pid=7661, port=5000, log=/home/nbs/Desktop/open-lambda/default-ol/worker.out
ready
Kill worker process with PID 7661

real	0m0.223s
user	0m0.000s
sys	0m0.021s

So, explicitly closing the connection. Refer https://stackoverflow.com/questions/10115126/python-requests-close-http-connection .
Explictly checking if the file open-lambda/default-ol/worker/worker.pid has been removed or not. But since, the file only has root permissions os.path.isfile() returns false.
So, using sudo find command to do the above. Refer https://stackoverflow.com/questions/3503879/assign-output-of-os-system-to-a-variable-and-prevent-it-from-being-displayed-on
Still, not helping usually fails after 5 to 10 iterations and gives following message.
Kill worker process with PID 5045
os: process already finished
Failed to kill process with PID 5045.  May require manual cleanup.

Removing default-ol directory gives following error
root@ubuntu:/home/nbs/Desktop/open-lambda# rm -rf default-ol/
rm: cannot remove 'default-ol/worker/root-sandboxes': Device or resource busy

Refer https://unix.stackexchange.com/questions/11238/how-to-get-over-device-or-resource-busy

lsof command gives following output
root@ubuntu:/home/nbs/Desktop/open-lambda# lsof +D .
lsof: WARNING: can't stat() fuse.gvfsd-fuse file system /run/user/1000/gvfs
      Output information may be incomplete.
COMMAND  PID USER   FD   TYPE DEVICE SIZE/OFF   NODE NAME
bash    4250  nbs  cwd    DIR    8,1     4096 677502 .
sudo    4284 root  cwd    DIR    8,1     4096 677502 .
su      4285 root  cwd    DIR    8,1     4096 677502 .
bash    4286 root  cwd    DIR    8,1     4096 677502 .
lsof    6293 root  cwd    DIR    8,1     4096 677502 .
lsof    6294 root  cwd    DIR    8,1     4096 677502 .

So, simply appending the new stats and restarting the VM each time it fails.

Taking stats for CGI without inserting the module :

./oracle.py -v -gr 100 7 10 http://localhost:8080/fibo/fibo.py cgipyjson.csv
./oracle.py -v -gr 100 7 10 http://localhost:8080/fibo/fibo.py cgipyqry.csv

fibo.py was taking time in ~80 ms but prntenv was taking in ~20 ms. Tried both by passing argument as query as well as body.
So, using fibo.go and building it statically and now its also taking ~20 ms using time(curl).
Even though, the time has reduced, its still not comparable to open-lambda's ~3ms. 
My speculation is that since our server is forking for each request, it is taking more time than open-lambda. 
Open-lambda is probably building the lambda once and then using it. 
Because, it is really astonishing that open-lambda uses python for lambda function and still keeps response time ~7 ms.

nbs@ubuntu:~/Desktop/project/Intercepting_Selective_System_Calls/src$ time (make)
# make -C /lib/modules/4.15.0-88-generic/build M=/home/nbs/Desktop/project/Intercepting_Selective_System_Calls/src modules
make -C /usr/src/linux-headers-4.15.0-88-generic SUBDIRS=/home/nbs/Desktop/project/Intercepting_Selective_System_Calls/src KBUILD_EXTMOD=/home/nbs/Desktop/project/Intercepting_Selective_System_Calls/src modules
make[1]: Entering directory '/usr/src/linux-headers-4.15.0-88-generic'
  CC [M]  /home/nbs/Desktop/project/Intercepting_Selective_System_Calls/src/iallops.o
  CC [M]  /home/nbs/Desktop/project/Intercepting_Selective_System_Calls/src/custmap.o
  LD [M]  /home/nbs/Desktop/project/Intercepting_Selective_System_Calls/src/kiallops.o
  Building modules, stage 2.
  MODPOST 1 modules
  CC      /home/nbs/Desktop/project/Intercepting_Selective_System_Calls/src/kiallops.mod.o
  LD [M]  /home/nbs/Desktop/project/Intercepting_Selective_System_Calls/src/kiallops.ko
make[1]: Leaving directory '/usr/src/linux-headers-4.15.0-88-generic'

real	0m4.086s
user	0m3.634s
sys	0m0.544s

nbs@ubuntu:~/Desktop/project/Intercepting_Selective_System_Calls/src$ time (make insmod)
sudo insmod kiallops.ko

real	0m0.037s
user	0m0.006s
sys	0m0.018s

nbs@ubuntu:~/Desktop/project/Intercepting_Selective_System_Calls/src$ time (make rmmod)
sudo rmmod kiallops.ko

real	0m0.031s
user	0m0.007s
sys	0m0.009s

nbs@ubuntu:~/Desktop/project/Intercepting_Selective_System_Calls/src$ time (make clean)
#make -C /lib/modules/4.15.0-88-generic/build M=/home/nbs/Desktop/project/Intercepting_Selective_System_Calls/src clean
make -C /usr/src/linux-headers-4.15.0-88-generic SUBDIRS=/home/nbs/Desktop/project/Intercepting_Selective_System_Calls/src clean
make[1]: Entering directory '/usr/src/linux-headers-4.15.0-88-generic'
  CLEAN   /home/nbs/Desktop/project/Intercepting_Selective_System_Calls/src/.tmp_versions
  CLEAN   /home/nbs/Desktop/project/Intercepting_Selective_System_Calls/src/Module.symvers
make[1]: Leaving directory '/usr/src/linux-headers-4.15.0-88-generic'

real	0m0.477s
user	0m0.357s
sys	0m0.194s

nbs@ubuntu:~/Desktop/project/CGI Controller/isolcon/fibo$ time (make)
CGO_ENABLED=0 GOOS=linux go build -a -ldflags '-extldflags "-static"' .

real	0m3.106s
user	0m5.018s
sys	0m0.490s

Above user time > real time. Probably due to multiple processors being used. Refer https://unix.stackexchange.com/questions/40694/why-real-time-can-be-lower-than-user-time

./oracle.py -v -gr 100 7 10 http://localhost:8080/fibo/fibo cgi.csv

This produced results in ~6ms which is far less than measured by time(curl).

Taking stats for CGI with module inserted :

Insert the module and then start the cgi controller.

./oracle.py -v -gr 100 7 10 http://localhost:8080/fibo/fibo ni.csv

Plotting graphs :

./oracle.py -v -gg 'cs.csv,ol.csv,ni.csv' 'Open Lambda (CS),Open Lambda (NO),Novo Isolates'
./oracle.py -v -gg 'ol.csv,ni.csv' 'Open Lambda,Novo Isolates'
./oracle.py -v -gg 'cgi.csv,ni.csv' 'Before Intercepting,After Intercepting'
./oracle.py -v -gg 'cs.csv,ol.csv' 'Cold Start,Normal Operation'
./oracle.py -v -gg 'cgipyjson.csv,cgipyqry.csv,cgi.csv' 'Python (JSON Body),Python (Query Params), Go (JSON & Static Build)'

After this all the plots were put in plots folder and csv files put in data folder.

Week 21 :

Writing Master's Thesis.

Points mentioned by Bishakh sir.
1. What is serverless cloud computing.
2. Current uses of serverless, uses container, why container used, whats it role.
3. Properties of serverless.
4. Using only process then what are the problems in a multi tenant system. Needs isolation, whatever we are doing.
5. What all we did, which system call we blocked, how many system calls were there, etc. Which ones whitelisted so statically compiled code was running. Why statically compiling is important.
6. Why require CGI for interfacing. CGI is a protocol, easy for input, output. We need a protocol for input output and that protocol is CGI, that's all. Similar to AWS, we will take the user's program and put a wrapper over it, by providing user with an interface for input output like in AWS lambda.

Need to write how serverless works, how container is created, why container is needed, etc. Its a multi tenant system, for which containers are important. Serverless properties : stateless, what all is maintained or allowed or not like memory maintenance, file system data, creation of sockets, etc. Multi tenant system : Single system runs processors for multiple users.

Refer Meeting with Sandip sir.

Skeleton of the paper :

1. Introduction 

What is serverless?

Properties of serverless : https://www.infoq.com/news/2019/08/traits-serverless-architecture/

Types of serverless implementation : https://www.cloudflare.com/learning/serverless/serverless-vs-containers/
a. Micro VMs implementation : refer Firecracker paper, used in AWS Lambda and Fargate.
b. Cloudflare Isolates implementation : uses V8 engine, refer cloudflare.
c. Container implementation : Used by many open source implementations, refer SAND, OpenLambda.

Uses of serverless : Refer COMSNETS Paper

Multi tenant architecture, advantages, disadvantages : https://www.datamation.com/cloud-computing/what-is-multi-tenant-architecture.html // Need to add

2. Related Works

Literature Survey : Tell about what current implementations are in place like MicroVMs used by firecracker, containers used by openlambda, isolates used by cloudflare : refer https://medium.com/@zackbloom/isolates-are-the-future-of-cloud-computing-cf7ab91c6142 , sand reduces latency for lambda functions belonging to same application by using two techniques, agile cold starts to reduce cold start time and finds that network setup contributes majorly to container startup time, SOCK uses 3-tier caching to optimize container starting time, Peeking Behind the curtains provides extensive comparision between current commercial lambda providers, ephemeral storage for serverless analytics.

// Ephemeral storage and caching techniques not added

Research Gap : Most use containers : problem of cold starts, vms : heavy & cold start, isolates : cloud flare but not language agnostic. There have been major strides in reducing cold start time in paper agile, sock, but still not that much.
None of the works so far have completely omitted cold start time, the first request always has to suffer some extra latency against subsequent request.
Even though multitenant solutions like isolates exist (cite cloudflare), it uses chrome's v8 engine which runs only javascript and hence it is not language agnostic like vm and container solutions.

3. Objective

Problem Motivation : Mention all the things in research gap. 

// As the isolation is done on the process level, it would be faster, even with database applications as then we can use the database on the same vm and intercept its calls, cite workshop paper. This was not added.

Problem Statement : To design a serverless cloud computing system which removes the problem of cold starts completely. To design a serverless cloud computing system which is language agnostic. To design a multitenant system which is lightweight in terms of bootstrapping, memory and computation and works at the process level. To analyse this noval system with respect to conventional serverless implementations in terms of cold start latency and performance.

Contributions : We have created a loadable kernel module which can intercept all system calls based on the parent pid of a process. We have created a CGI server in GoLang which is language agnostic as it can run CGI scripts written in any language. We have created a serverless cloud computing system called as Novo Isolates which takes as input statically compiled CGI scripts and has 0 cold start latency and works at the process level. We have done following latency analysis : OpenLambda's cold start and normal operation and novoisolates's Normal operation, Between Normal operation time of OpenLambda and NovoIsolates, Effects of intercepting system calls on latency, When using different language CGI scripts, tabulating various components of time for bootstrapping the system, running, etc.

4. Proposed Model

Serverless cloud computing is an amazing architecture where the users don't have to bother about how to deploy the servers and only have to pay for the time servers remain active for serving their client's requests. But, still this model suffers from cold start latency. Multiple solutions exist which try to minimise or omit this problem but they come with their own disadvantages. Here, we propose a system which mitigates this problem completely without affecting the latency of serving the requests. Our model consists of two parts : the loadable kernel module, and a CGI Server.

lkm : We have created a loadable kernel module in linux which is capable of selectively intercepting system calls made by a process based on its parent's pid. While creating this module, we faced several difficulties and we came up with solutons to handle them step by step. We describe these problems and solution in detail below.

4.1.1 Getting the address of system call table.
4.1.2 Logging file and network operations and then all system calls. mention problem while logging all, why chose to use statically compiled binaries. blocked all syscalls and whitelisted a few.
4.1.3 Intercepting selective system calls. using parent pid as a parameter

cgi : We needed a protocol for the application creaters to follow so that their clients can easily send their requests. For this, we used the CGI protocol. It is this server whose pid is passed as parent pid to the lkm for selective intercepting of system calls. It has following features : It can run binary of any language in general but to allow certain operations that binary must be statically compiled, For each request it forks a new process in the directory of that application binary thereby running each request as an isolated process, The server's bootstrapping time is minimal as it is itself a process.

5. Experiments, Results and Analysis

fibonacci : to reduce variability, we have only taken input uptil integer 10 and we provide the same seed value 7 for each experiment.

One of the things that makes our system stand out is it being language agnostic in nature.

6. Conclusion and future works

Conclusion : mention all the things in contributions
Not only does our system solves the cold start problem, but it is also language agnostic.
Future works : We want to do a detailed security analysis of the system, We want to test this system and see if its commercially viable, We want to add components to it for monitoring the requests being handled in the system, We want to compare it with existing commercially available systems. We want to reduce normal operation time further. We want to test a hybrid system where the novoisolates works over containers or VMs, this would be helpful in doing application level sandboxing where all the lambdas of an application lies within one container or VM.

Week 22 :

Completing report and making presentation.

Week 23 :

Presentation done. 8 June 2020.

Tips for presentation :

Modifying kernel ? No, providing an application, doesn’t require any kernel update, providing a lkm.
Isolate the environment for the process, isolation for VM happens this way.
Why intercepting sys calls? Don’t go directly to lkm.
Why not use any other process level method? Can handle everything by modifying glibc, but some other process might not use glibc like go, etc.
It will reduce the computation —> cost of servers reduced —> reduce the power usage —> monetary value, cold start 2 things - user sees delay, server running 24 hours, more power consumed and cooling.
Try to connect, if not connecting, start from some slides back.
CGI = Common Gateway Interface
Static loaded library done, dynamically loaded library how? We haven’t done it yet.
Defend yourself but not lie. Say that not completed due to time constraint but say that this much done.
Don’t say you don’t remember. Can improvise. Tell truth. Be confident. Answer to the best knowledge.

28 June 2020 :

Comparing memory used in isolates vs openlambda
Measuring memory used before and after starting the containers.
Memory is in MBs. Used memory before starting openlambda and novoisolates saved in um1.csv and um2.csv respectively. Used memory after starting them saved in umol.csv and umni.csv respectively.

for run in {1..10}; do free -m | grep Mem | awk '{print $3}'; done > um.csv

./oracle.py -v -gg 'data/mol.csv,data/mni.csv' 'Open Lambda, Novo Isolates'

Done.